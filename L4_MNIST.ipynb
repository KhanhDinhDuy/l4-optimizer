{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4 stepsize adaptation performance on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook contains a minimum working example of L4 optimizers [Rolinek, Martius 2018](https://arxiv.org/abs/1802.05074) performing on the classical MNIST dataset.\n",
    "If you find bugs or make improvements: [pull requests welcome](https://github.com/martius-lab/l4-optimizer/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:10:31.336385Z",
     "start_time": "2018-02-20T17:10:31.325392Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "import L4.L4 as L4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:10:36.230497Z",
     "start_time": "2018-02-20T17:10:36.215673Z"
    }
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden=(300,100), num_output=10):\n",
    "    in_dim = x.get_shape().as_list()[1]\n",
    "    y_layer = x\n",
    "    for l,n in enumerate(hidden):\n",
    "        W = tf.get_variable(\"W{}\".format(l), [in_dim, n],\n",
    "                            initializer=layers.xavier_initializer())\n",
    "        b = tf.get_variable(\"b{}\".format(l), [n],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        y_layer = tf.nn.relu(tf.matmul(y_layer, W) + b)\n",
    "        in_dim = n\n",
    "    W = tf.get_variable(\"W_final\", [in_dim, num_output],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    b = tf.get_variable(\"b_final\", [num_output],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    y = tf.matmul(y_layer, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:10:37.827488Z",
     "start_time": "2018-02-20T17:10:37.824356Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {'data_dir': 'data',\n",
    "          'epochs': 35,\n",
    "          'batch_size': 64,\n",
    "          'hidden': [300, 100],\n",
    "          'epochs_per_report': 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:10:45.795608Z",
     "start_time": "2018-02-20T17:10:44.327630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST_size = 60000\n",
    "mnist = input_data.read_data_sets(config['data_dir'], one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = mlp(x, hidden=config['hidden'], num_output=10)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-19T23:18:29.654940",
     "start_time": "2018-02-19T23:18:29.646661"
    }
   },
   "source": [
    "Choose an optimizer: for L4 you have the choice between L4Adam and L4Mom. Without arguments the default parameters are used, e.g. fraction=0.15\n",
    "\n",
    "The hyperparameters (if specified) are the best for achieving minimal loss after ~32 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:10:51.073582Z",
     "start_time": "2018-02-20T17:10:50.943453Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = L4.L4Adam(fraction=0.25) # (L4Adam)\n",
    "#opt = L4.L4Adam() # default value (L4Adam*)\n",
    "#opt = L4.L4Mom(fraction=0.25) # (L4Mom)\n",
    "#opt = tf.train.AdamOptimizer(0.001, epsilon=1e-4) # (Adam)\n",
    "#opt = tf.train.MomentumOptimizer(learning_rate=0.05, momentum=0.9) # (mSGD)\n",
    "#opt = tf.train.GradientDescentOptimizer(learning_rate=0.7) # (SGD)\n",
    "\n",
    "train_op = opt.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-20T17:12:45.146386Z",
     "start_time": "2018-02-20T17:10:59.951343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Current Batch Loss: 2.3025853633880615\n",
      "Epoch 1; Current Batch Loss: 0.18231457471847534\n",
      "Epoch 2; Current Batch Loss: 0.02866493910551071\n",
      "Epoch 3; Current Batch Loss: 0.07279851287603378\n",
      "Epoch 4; Current Batch Loss: 0.025335367769002914\n",
      "Epoch 5; Current Batch Loss: 0.002143927151337266\n",
      "Epoch 6; Current Batch Loss: 0.017017511650919914\n",
      "Epoch 7; Current Batch Loss: 0.00016670141485519707\n",
      "Epoch 8; Current Batch Loss: 6.159079475764884e-06\n",
      "Epoch 9; Current Batch Loss: 0.002517925575375557\n",
      "Epoch 10; Current Batch Loss: 5.092173978482606e-06\n",
      "Epoch 11; Current Batch Loss: 6.684324944217224e-06\n",
      "Epoch 12; Current Batch Loss: 1.1290010661468841e-05\n",
      "Epoch 13; Current Batch Loss: 2.963222868856974e-06\n",
      "Epoch 14; Current Batch Loss: 5.531650458578952e-06\n",
      "Epoch 15; Current Batch Loss: 2.998848458446446e-07\n",
      "Epoch 16; Current Batch Loss: 0.0\n",
      "Epoch 17; Current Batch Loss: 3.520362952258438e-07\n",
      "Epoch 18; Current Batch Loss: 9.313223969797946e-09\n",
      "Epoch 19; Current Batch Loss: 3.3713513403199613e-07\n",
      "Epoch 20; Current Batch Loss: 9.313223081619526e-09\n",
      "Epoch 21; Current Batch Loss: 1.8626450382086546e-09\n",
      "Epoch 22; Current Batch Loss: 0.0\n",
      "Epoch 23; Current Batch Loss: 0.0\n",
      "Epoch 24; Current Batch Loss: 0.0\n",
      "Epoch 25; Current Batch Loss: 1.8626450382086546e-09\n",
      "Epoch 26; Current Batch Loss: 0.0\n",
      "Epoch 27; Current Batch Loss: 0.0\n",
      "Epoch 28; Current Batch Loss: 0.0\n",
      "Epoch 29; Current Batch Loss: 0.0\n",
      "Epoch 30; Current Batch Loss: 0.0\n",
      "Epoch 31; Current Batch Loss: 0.0\n",
      "Epoch 32; Current Batch Loss: 0.0\n",
      "Epoch 33; Current Batch Loss: 0.0\n",
      "Epoch 34; Current Batch Loss: 0.0\n",
      "Epoch 35; Current Batch Loss: 0.0\n",
      "Test accuracy: 0.9839000105857849\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()    \n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "batches_per_epoch = (MNIST_size // config['batch_size'])\n",
    "batches_to_run = config['epochs'] * batches_per_epoch\n",
    "\n",
    "for b in range(batches_to_run+1):    \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(config['batch_size'])\n",
    "    _, loss = sess.run((train_op, cross_entropy), feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    if b % batches_per_epoch == 0:\n",
    "        epoch_nr = b // batches_per_epoch\n",
    "        if epoch_nr % config['epochs_per_report'] == 0:\n",
    "            print(\"Epoch {}; Current Batch Loss: {}\".format(epoch_nr, loss))\n",
    "\n",
    "# Test trained model\n",
    "accuracy_value = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(\"Test accuracy: {}\".format(accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
