{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L4 stepsize adaptation performance on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short notebook contains a minimum working example of L4 optimizers (Rolinek, Martius 2018) performing on the classical MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:17:28.219532Z",
     "start_time": "2018-02-12T13:17:27.141770Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import layers\n",
    "\n",
    "import L4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:17:28.231487Z",
     "start_time": "2018-02-12T13:17:28.221109Z"
    }
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden=(300,100), num_output=10):\n",
    "    in_dim = x.get_shape().as_list()[1]\n",
    "    y_layer = x\n",
    "    for l,n in enumerate(hidden):\n",
    "        W = tf.get_variable(\"W{}\".format(l), [in_dim, n],\n",
    "                            initializer=layers.xavier_initializer())\n",
    "        b = tf.get_variable(\"b{}\".format(l), [n],\n",
    "                            initializer=tf.zeros_initializer())\n",
    "        y_layer = tf.nn.relu(tf.matmul(y_layer, W) + b)\n",
    "        in_dim = n\n",
    "    W = tf.get_variable(\"W_final\", [in_dim, num_output],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    b = tf.get_variable(\"b_final\", [num_output],\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    y = tf.matmul(y_layer, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:17:28.237243Z",
     "start_time": "2018-02-12T13:17:28.232999Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {'data_dir': 'data',\n",
    "          'epochs': 35,\n",
    "          'batch_size': 64,\n",
    "          'hidden': [300, 100],\n",
    "          'epochs_per_report': 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:17:28.641052Z",
     "start_time": "2018-02-12T13:17:28.238745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST_size = 60000\n",
    "mnist = input_data.read_data_sets(config['data_dir'], one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = mlp(x, hidden=config['hidden'], num_output=10)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:17:28.798945Z",
     "start_time": "2018-02-12T13:17:28.642138Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = L4.L4Adam(fraction=0.25)\n",
    "#opt = L4.L4Mom(fraction=0.25)\n",
    "#opt = tf.train.AdamOptimizer(0.001, epsilon=1e-4)\n",
    "#opt = tf.train.MomentumOptimizer(learning_rate=0.05, momentum=0.9)\n",
    "#opt = tf.train.GradientDescentOptimizer(learning_rate=0.7)\n",
    "\n",
    "train_op = opt.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-12T13:19:22.591130Z",
     "start_time": "2018-02-12T13:17:28.800133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Current Batch Loss: 2.3025853633880615\n",
      "Epoch 1; Current Batch Loss: 0.05435074120759964\n",
      "Epoch 2; Current Batch Loss: 0.0886421799659729\n",
      "Epoch 3; Current Batch Loss: 0.049092553555965424\n",
      "Epoch 4; Current Batch Loss: 0.08038255572319031\n",
      "Epoch 5; Current Batch Loss: 0.0029454075265675783\n",
      "Epoch 6; Current Batch Loss: 0.0061236158944666386\n",
      "Epoch 7; Current Batch Loss: 0.00017982714052777737\n",
      "Epoch 8; Current Batch Loss: 0.00031989847775548697\n",
      "Epoch 9; Current Batch Loss: 0.0009350153268314898\n",
      "Epoch 10; Current Batch Loss: 8.277507004095241e-05\n",
      "Epoch 11; Current Batch Loss: 2.9688142149097985e-06\n",
      "Epoch 12; Current Batch Loss: 7.34933273633942e-05\n",
      "Epoch 13; Current Batch Loss: 1.6763797461294416e-08\n",
      "Epoch 14; Current Batch Loss: 1.8626450382086546e-09\n",
      "Epoch 15; Current Batch Loss: 3.1664953326071554e-08\n",
      "Epoch 16; Current Batch Loss: 5.196699248699588e-07\n",
      "Epoch 17; Current Batch Loss: 1.1175867342672063e-08\n",
      "Epoch 18; Current Batch Loss: 5.401665958970625e-08\n",
      "Epoch 19; Current Batch Loss: 0.0\n",
      "Epoch 20; Current Batch Loss: 1.8626450382086546e-09\n",
      "Epoch 21; Current Batch Loss: 0.0\n",
      "Epoch 22; Current Batch Loss: 0.0\n",
      "Epoch 23; Current Batch Loss: 0.0\n",
      "Epoch 24; Current Batch Loss: 0.0\n",
      "Epoch 25; Current Batch Loss: 0.0\n",
      "Epoch 26; Current Batch Loss: 0.0\n",
      "Epoch 27; Current Batch Loss: 0.0\n",
      "Epoch 28; Current Batch Loss: 0.0\n",
      "Epoch 29; Current Batch Loss: 0.0\n",
      "Epoch 30; Current Batch Loss: 0.0\n",
      "Epoch 31; Current Batch Loss: 0.0\n",
      "Epoch 32; Current Batch Loss: 0.0\n",
      "Epoch 33; Current Batch Loss: 0.0\n",
      "Epoch 34; Current Batch Loss: 0.0\n",
      "Epoch 35; Current Batch Loss: 0.0\n",
      "Test accuracy: 0.9830999970436096\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()    \n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "batches_per_epoch = (MNIST_size // config['batch_size'])\n",
    "batches_to_run = config['epochs'] * batches_per_epoch\n",
    "\n",
    "for b in range(batches_to_run+1):    \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(config['batch_size'])\n",
    "    _, loss = sess.run((train_op, cross_entropy), feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    if b % batches_per_epoch == 0:\n",
    "        epoch_nr = b // batches_per_epoch\n",
    "        if epoch_nr % config['epochs_per_report'] == 0:\n",
    "            print(\"Epoch {}; Current Batch Loss: {}\".format(epoch_nr, loss))\n",
    "\n",
    "# Test trained model\n",
    "accuracy_value = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(\"Test accuracy: {}\".format(accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
